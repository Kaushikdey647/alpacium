{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a92ff3c",
   "metadata": {},
   "source": [
    "# Neural Network Fitter for Alpha Strategy Optimization\n",
    "\n",
    "This notebook documents the design and implementation of a neural network-based parameter optimization engine for trading strategies. The goal is to create a system that can automatically tune strategy parameters to optimize for PnL and Sharpe ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d775f1",
   "metadata": {},
   "source": [
    "# 1. Requirements Analysis and Input/Output Design\n",
    "\n",
    "## Problem Statement\n",
    "We need to create a neural network-based engine that can optimize the parameters of trading strategies by learning from historical data and backtest results. The initial use case is optimizing RSI strategy parameters, but the system should be flexible enough to handle other strategies.\n",
    "\n",
    "## Input Requirements\n",
    "1. Historical Data:\n",
    "   - Multi-index DataFrame with (symbol, timestamp) index\n",
    "   - OHLCV data for each symbol\n",
    "   - Same format as required by existing AlphaEngine\n",
    "\n",
    "2. Alpha Strategy:\n",
    "   - The alpha strategy function (e.g., rsi_reversion)\n",
    "   - Default parameters and their ranges\n",
    "   - Parameter types and constraints\n",
    "\n",
    "3. Initial Parameters:\n",
    "   - For RSI strategy:\n",
    "     - rsi_period: int [5-50]\n",
    "     - overbought: float [50-90]\n",
    "     - oversold: float [10-50]\n",
    "     - smoothing: int [1-10]\n",
    "\n",
    "4. Training Configuration:\n",
    "   - Training/validation split\n",
    "   - Optimization metrics weights (PnL vs Sharpe)\n",
    "   - Training hyperparameters\n",
    "\n",
    "## Output Requirements\n",
    "1. Optimized Parameters:\n",
    "   - Dictionary of optimized parameter values\n",
    "   - Parameter history during optimization\n",
    "   - Confidence metrics for each parameter\n",
    "\n",
    "2. Performance Metrics:\n",
    "   - Training history\n",
    "   - Backtest results with optimized parameters\n",
    "   - Comparison with baseline parameters\n",
    "\n",
    "## Key Challenges\n",
    "1. Non-differentiable backtesting process\n",
    "2. Complex relationship between parameters and performance\n",
    "3. Need for parameter constraints and validation\n",
    "4. Balancing multiple optimization objectives (PnL, Sharpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff238213",
   "metadata": {},
   "source": [
    "# 2. Machine Learning Solution Design\n",
    "\n",
    "## Approach Options\n",
    "\n",
    "### Option 1: Direct Parameter Prediction\n",
    "- Input: Market state features\n",
    "- Output: Optimal parameters for next period\n",
    "- Pros: Simple architecture, direct optimization\n",
    "- Cons: May not capture temporal dependencies well\n",
    "\n",
    "### Option 2: Reinforcement Learning\n",
    "- State: Market conditions + current parameters\n",
    "- Action: Parameter adjustments\n",
    "- Reward: PnL and Sharpe ratio\n",
    "- Pros: Natural fit for sequential decision making\n",
    "- Cons: Complex training, stability issues\n",
    "\n",
    "### Option 3: Hybrid Architecture (Recommended)\n",
    "- Combines supervised learning with reinforcement learning\n",
    "- Two-stage process:\n",
    "  1. Parameter initialization network (supervised)\n",
    "  2. Parameter refinement network (RL)\n",
    "- Pros: Stable training, better exploration\n",
    "- Cons: More complex implementation\n",
    "\n",
    "## Input Feature Design\n",
    "\n",
    "1. Market State Features:\n",
    "   - Price momentum at multiple timeframes\n",
    "   - Volatility metrics\n",
    "   - Volume profiles\n",
    "   - Market regime indicators\n",
    "\n",
    "2. Parameter State Features:\n",
    "   - Current parameter values\n",
    "   - Parameter gradients from previous iterations\n",
    "   - Performance metrics with current parameters\n",
    "\n",
    "3. Performance Metrics:\n",
    "   - Rolling PnL\n",
    "   - Rolling Sharpe ratio\n",
    "   - Rolling turnover\n",
    "   - Maximum drawdown\n",
    "\n",
    "## Output Structure\n",
    "\n",
    "1. Parameter Updates:\n",
    "   - Direct values for each parameter\n",
    "   - Scaling factors to apply to current parameters\n",
    "   - Confidence scores for each prediction\n",
    "\n",
    "2. Auxiliary Outputs:\n",
    "   - Expected performance metrics\n",
    "   - Parameter stability indicators\n",
    "   - Market regime classification\n",
    "\n",
    "## Training Strategy\n",
    "\n",
    "1. Initial Phase:\n",
    "   - Supervised pretraining on historical data\n",
    "   - Learn parameter mappings for different market regimes\n",
    "\n",
    "2. Fine-tuning Phase:\n",
    "   - Online learning with reinforcement\n",
    "   - Adaptive parameter updates based on performance\n",
    "\n",
    "3. Validation:\n",
    "   - Out-of-sample backtesting\n",
    "   - Parameter stability analysis\n",
    "   - Performance attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40cc5ad",
   "metadata": {},
   "source": [
    "# 3. Neural Network Architecture Design\n",
    "\n",
    "## Network Components\n",
    "\n",
    "### 1. Feature Extraction Module\n",
    "```python\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.conv1d = nn.Conv1d(input_dim, 64, kernel_size=3)\n",
    "        self.lstm = nn.LSTM(64, 128, num_layers=2, dropout=0.1)\n",
    "        self.attention = MultiHeadAttention(128, num_heads=4)\n",
    "```\n",
    "\n",
    "- Input dimension: 5 (OHLCV) + derived features\n",
    "- Convolutional layers for local pattern detection\n",
    "- LSTM layers for temporal dependencies\n",
    "- Multi-head attention for market regime focus\n",
    "\n",
    "### 2. Parameter Prediction Module\n",
    "```python\n",
    "class ParameterPredictor(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.param_heads = nn.ModuleDict({\n",
    "            'rsi_period': nn.Linear(128, 1),\n",
    "            'overbought': nn.Linear(128, 1),\n",
    "            'oversold': nn.Linear(128, 1),\n",
    "            'smoothing': nn.Linear(128, 1)\n",
    "        })\n",
    "```\n",
    "\n",
    "- Separate prediction heads for each parameter\n",
    "- Parameter-specific activation functions\n",
    "- Range constraints via sigmoid/softplus\n",
    "\n",
    "### 3. Performance Prediction Module\n",
    "```python\n",
    "class PerformancePredictor(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.metrics = nn.Linear(64, 3)  # PnL, Sharpe, Turnover\n",
    "```\n",
    "\n",
    "- Predicts expected performance metrics\n",
    "- Used for parameter validation\n",
    "- Helps in early stopping\n",
    "\n",
    "## Layer Sizes and Activation Functions\n",
    "\n",
    "1. Feature Extraction:\n",
    "   - Conv1D: in_channels=5, out_channels=64, kernel_size=3\n",
    "   - LSTM: input_size=64, hidden_size=128, num_layers=2\n",
    "   - Attention: embed_dim=128, num_heads=4\n",
    "\n",
    "2. Parameter Prediction:\n",
    "   - FC1: 128 → 256 (ReLU)\n",
    "   - FC2: 256 → 128 (ReLU)\n",
    "   - Parameter Heads: 128 → 1 (Custom activation)\n",
    "\n",
    "3. Performance Prediction:\n",
    "   - FC1: 128 → 128 (ReLU)\n",
    "   - FC2: 128 → 64 (ReLU)\n",
    "   - Metrics: 64 → 3 (Linear)\n",
    "\n",
    "## Custom Components\n",
    "\n",
    "### 1. Multi-Head Attention\n",
    "```python\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            embed_dim, num_heads, dropout=0.1\n",
    "        )\n",
    "```\n",
    "\n",
    "### 2. Parameter Constraints\n",
    "```python\n",
    "class ParameterConstraints(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.constraints = {\n",
    "            'rsi_period': (5, 50),\n",
    "            'overbought': (50, 90),\n",
    "            'oversold': (10, 50),\n",
    "            'smoothing': (1, 10)\n",
    "        }\n",
    "```\n",
    "\n",
    "### 3. Loss Components\n",
    "```python\n",
    "class HybridLoss(nn.Module):\n",
    "    def __init__(self, pnl_weight=0.5, sharpe_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.pnl_weight = pnl_weight\n",
    "        self.sharpe_weight = sharpe_weight\n",
    "```\n",
    "\n",
    "## Integration Architecture\n",
    "```python\n",
    "class NeuralAlphaOptimizer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = FeatureExtractor(input_dim)\n",
    "        self.param_predictor = ParameterPredictor(128)\n",
    "        self.perf_predictor = PerformancePredictor(128)\n",
    "        self.constraints = ParameterConstraints()\n",
    "```\n",
    "\n",
    "## Training Configuration\n",
    "\n",
    "1. Optimizer:\n",
    "   - Adam with learning rate scheduling\n",
    "   - Initial lr: 0.001\n",
    "   - Weight decay: 0.0001\n",
    "\n",
    "2. Batch Processing:\n",
    "   - Batch size: 32 days\n",
    "   - Sequence length: 252 days\n",
    "   - Sliding window with 126 day overlap\n",
    "\n",
    "3. Loss Weights:\n",
    "   - PnL component: 0.5\n",
    "   - Sharpe ratio component: 0.5\n",
    "   - Parameter stability regularization: 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Engine Interface Design and Implementation\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Callable, Any, Dict, List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from src.engines.alpha_engine import AlphaSignals\n",
    "from src.engines.backtesting_engine import BacktestingEngine, SimulationResults\n",
    "\n",
    "@dataclass\n",
    "class OptimizationResults:\n",
    "    \"\"\"Container for optimization results\"\"\"\n",
    "    optimized_parameters: Dict[str, Any]\n",
    "    training_history: pd.DataFrame\n",
    "    backtest_results: SimulationResults\n",
    "    parameter_history: List[Dict[str, Any]]\n",
    "    confidence_scores: Dict[str, float]\n",
    "    \n",
    "class NeuralNetworkFitter:\n",
    "    \"\"\"Engine for optimizing alpha strategy parameters using neural networks\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        backtesting_engine: BacktestingEngine,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.backtesting_engine = backtesting_engine\n",
    "        self.optimizer = torch.optim.Adam(model.parameters())\n",
    "        self._validation_window = 126  # ~6 months\n",
    "        \n",
    "    def optimize_parameters(\n",
    "        self,\n",
    "        historical_data: pd.DataFrame,\n",
    "        alpha_function: Callable[..., pd.DataFrame],\n",
    "        initial_parameters: Dict[str, Any],\n",
    "        n_iterations: int = 100,\n",
    "        learning_rate: float = 0.001,\n",
    "        pnl_weight: float = 0.5,\n",
    "        sharpe_weight: float = 0.5,\n",
    "        early_stopping_patience: int = 10,\n",
    "        show_progress: bool = True\n",
    "    ) -> OptimizationResults:\n",
    "        \"\"\"\n",
    "        Optimize strategy parameters using neural network predictions.\n",
    "        \n",
    "        Args:\n",
    "            historical_data: Multi-index DataFrame with market data\n",
    "            alpha_function: The alpha strategy to optimize\n",
    "            initial_parameters: Starting parameter values\n",
    "            n_iterations: Number of optimization iterations\n",
    "            learning_rate: Learning rate for parameter updates\n",
    "            pnl_weight: Weight for PnL in optimization objective\n",
    "            sharpe_weight: Weight for Sharpe ratio in optimization\n",
    "            early_stopping_patience: Iterations before early stopping\n",
    "            show_progress: Whether to show progress bar\n",
    "        \n",
    "        Returns:\n",
    "            OptimizationResults with optimized parameters and metrics\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _prepare_features(\n",
    "        self, \n",
    "        historical_data: pd.DataFrame\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Prepare input features for the neural network\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _validate_parameters(\n",
    "        self, \n",
    "        parameters: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Ensure parameters meet constraints\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _calculate_objective(\n",
    "        self,\n",
    "        backtest_results: SimulationResults,\n",
    "        pnl_weight: float,\n",
    "        sharpe_weight: float\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Calculate optimization objective from backtest results\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def save_model(\n",
    "        self,\n",
    "        filepath: str\n",
    "    ) -> None:\n",
    "        \"\"\"Save trained model and optimization state\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(\n",
    "        cls,\n",
    "        filepath: str,\n",
    "        backtesting_engine: BacktestingEngine\n",
    "    ) -> 'NeuralNetworkFitter':\n",
    "        \"\"\"Load trained model and optimization state\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b9ff54",
   "metadata": {},
   "source": [
    "# 5. Implementation Plan and Next Steps\n",
    "\n",
    "## Development Phases\n",
    "\n",
    "### Phase 1: Core Infrastructure\n",
    "1. Set up project structure:\n",
    "   - Create `engines/neural_network_fitter.py`\n",
    "   - Add tests in `tests/engines/test_neural_network_fitter.py`\n",
    "   - Create utility modules for feature extraction\n",
    "\n",
    "2. Implement base classes:\n",
    "   - NeuralNetworkFitter engine\n",
    "   - Custom PyTorch modules\n",
    "   - Data preprocessing utilities\n",
    "\n",
    "3. Create validation framework:\n",
    "   - Parameter validation\n",
    "   - Performance metrics tracking\n",
    "   - Early stopping logic\n",
    "\n",
    "### Phase 2: Model Development\n",
    "1. Implement neural network components:\n",
    "   - Feature extraction module\n",
    "   - Parameter prediction module\n",
    "   - Performance prediction module\n",
    "\n",
    "2. Create training pipeline:\n",
    "   - Data loading and batching\n",
    "   - Training loop\n",
    "   - Validation cycle\n",
    "\n",
    "3. Add optimization logic:\n",
    "   - Parameter constraints\n",
    "   - Gradient calculations\n",
    "   - Update strategies\n",
    "\n",
    "### Phase 3: Integration and Testing\n",
    "1. Integrate with existing engines:\n",
    "   - AlphaEngine integration\n",
    "   - BacktestingEngine integration\n",
    "   - Logging and monitoring\n",
    "\n",
    "2. Implement persistence:\n",
    "   - Model saving/loading\n",
    "   - Parameter history tracking\n",
    "   - Performance logging\n",
    "\n",
    "3. Create examples and documentation:\n",
    "   - Usage examples\n",
    "   - Parameter tuning guide\n",
    "   - Performance optimization tips\n",
    "\n",
    "## Testing Strategy\n",
    "\n",
    "1. Unit Tests:\n",
    "   - Parameter validation\n",
    "   - Feature extraction\n",
    "   - Model components\n",
    "\n",
    "2. Integration Tests:\n",
    "   - End-to-end optimization\n",
    "   - Backtest integration\n",
    "   - Performance metrics\n",
    "\n",
    "3. Performance Tests:\n",
    "   - Training speed\n",
    "   - Memory usage\n",
    "   - Optimization convergence\n",
    "\n",
    "## Documentation Requirements\n",
    "\n",
    "1. Class Documentation:\n",
    "   - Detailed docstrings\n",
    "   - Usage examples\n",
    "   - Parameter explanations\n",
    "\n",
    "2. Architecture Documentation:\n",
    "   - Component diagrams\n",
    "   - Data flow descriptions\n",
    "   - Integration points\n",
    "\n",
    "3. User Guide:\n",
    "   - Setup instructions\n",
    "   - Optimization examples\n",
    "   - Troubleshooting guide\n",
    "\n",
    "## Timeline and Milestones\n",
    "\n",
    "1. Week 1:\n",
    "   - Core infrastructure setup\n",
    "   - Basic model implementation\n",
    "   - Initial testing framework\n",
    "\n",
    "2. Week 2:\n",
    "   - Neural network implementation\n",
    "   - Training pipeline development\n",
    "   - Integration with existing engines\n",
    "\n",
    "3. Week 3:\n",
    "   - Testing and validation\n",
    "   - Documentation\n",
    "   - Performance optimization\n",
    "\n",
    "4. Week 4:\n",
    "   - User acceptance testing\n",
    "   - Performance tuning\n",
    "   - Production deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250e65d4",
   "metadata": {},
   "source": [
    "# 6. Enhanced System Design\n",
    "\n",
    "## Weights & Biases Integration\n",
    "\n",
    "### W&B Configuration\n",
    "```python\n",
    "@dataclass\n",
    "class WandBConfig:\n",
    "    project_name: str\n",
    "    entity: str\n",
    "    tags: List[str]\n",
    "    config: Dict[str, Any]\n",
    "    \n",
    "    def setup(self):\n",
    "        wandb.init(\n",
    "            project=self.project_name,\n",
    "            entity=self.entity,\n",
    "            tags=self.tags,\n",
    "            config=self.config\n",
    "        )\n",
    "```\n",
    "\n",
    "### Tracked Metrics\n",
    "1. Training Metrics:\n",
    "   - Loss components (PnL, Sharpe, Turnover)\n",
    "   - Parameter gradients\n",
    "   - Learning rate changes\n",
    "   \n",
    "2. Validation Metrics:\n",
    "   - Backtest performance\n",
    "   - Parameter stability\n",
    "   - Market regime detection\n",
    "\n",
    "3. Model Artifacts:\n",
    "   - Best performing models\n",
    "   - Parameter evolution\n",
    "   - Training configs\n",
    "\n",
    "## Market Configuration System\n",
    "\n",
    "### Market Config Interface\n",
    "```python\n",
    "@dataclass\n",
    "class MarketConfig:\n",
    "    universe: str  # USTOP100, USTOP200, USTOP500, USTOP1000\n",
    "    timeframe: str  # minute, hour, day\n",
    "    start_date: datetime\n",
    "    end_date: datetime\n",
    "    metrics: List[str]  # PnL, Sharpe, Turnover\n",
    "    constraints: Dict[str, Any]\n",
    "```\n",
    "\n",
    "### Universe-Specific Features\n",
    "1. USTOP100:\n",
    "   - Market cap weighted features\n",
    "   - Sector correlation features\n",
    "   - High liquidity metrics\n",
    "\n",
    "2. USTOP500:\n",
    "   - Cross-sector indicators\n",
    "   - Market breadth features\n",
    "   - Volume profile analysis\n",
    "\n",
    "3. Timeframe Adaptations:\n",
    "   - Minute: Microstructure features\n",
    "   - Hour: Intraday patterns\n",
    "   - Day: Overnight gaps, earnings impact\n",
    "\n",
    "## Deployable Model Interface\n",
    "\n",
    "### ModelInterface Class\n",
    "```python\n",
    "class AlphaModelInterface:\n",
    "    \"\"\"Interface for deploying optimized alpha models\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        market_config: MarketConfig,\n",
    "        update_frequency: str = \"daily\"\n",
    "    ):\n",
    "        self.model = self._load_model(model_path)\n",
    "        self.market_config = market_config\n",
    "        self.update_frequency = update_frequency\n",
    "        self._last_update = None\n",
    "        \n",
    "    def get_optimal_parameters(\n",
    "        self, \n",
    "        current_market_state: pd.DataFrame\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Get optimal parameters for current market state\"\"\"\n",
    "        \n",
    "    def update_model(\n",
    "        self, \n",
    "        new_market_data: pd.DataFrame\n",
    "    ) -> None:\n",
    "        \"\"\"Update model with new market data\"\"\"\n",
    "        \n",
    "    def validate_performance(\n",
    "        self\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Validate model performance\"\"\"\n",
    "```\n",
    "\n",
    "### Deployment Pipeline\n",
    "1. Model Export:\n",
    "   ```python\n",
    "   class ModelExporter:\n",
    "       def export_for_production(\n",
    "           self,\n",
    "           model: NeuralNetworkFitter,\n",
    "           market_config: MarketConfig\n",
    "       ) -> AlphaModelInterface:\n",
    "           \"\"\"Export model for production deployment\"\"\"\n",
    "   ```\n",
    "\n",
    "2. Real-time Integration:\n",
    "   ```python\n",
    "   class AlphaStrategyRunner:\n",
    "       def __init__(\n",
    "           self,\n",
    "           alpha_function: Callable,\n",
    "           model_interface: AlphaModelInterface\n",
    "       ):\n",
    "           self.alpha = alpha_function\n",
    "           self.model = model_interface\n",
    "           \n",
    "       def generate_signals(\n",
    "           self,\n",
    "           market_data: pd.DataFrame\n",
    "       ) -> pd.DataFrame:\n",
    "           \"\"\"Generate signals with optimized parameters\"\"\"\n",
    "   ```\n",
    "\n",
    "## Neural Network Adaptations\n",
    "\n",
    "### Market-Specific Architectures\n",
    "1. Large Universe (USTOP1000):\n",
    "   - Hierarchical attention networks\n",
    "   - Sector-level feature aggregation\n",
    "   - Sparse input processing\n",
    "\n",
    "2. High-Frequency (Minute):\n",
    "   - Temporal convolutional networks\n",
    "   - Real-time feature processing\n",
    "   - Adaptive parameter updates\n",
    "\n",
    "3. Market Regime Components:\n",
    "   - Regime detection subnet\n",
    "   - Parameter adaptation layers\n",
    "   - Cross-asset attention mechanism\n",
    "\n",
    "### Enhanced Feature Processing\n",
    "```python\n",
    "class MarketAwareFeatureExtractor(nn.Module):\n",
    "    def __init__(self, market_config: MarketConfig):\n",
    "        super().__init__()\n",
    "        self.market_config = market_config\n",
    "        self.features = self._build_feature_layers()\n",
    "        \n",
    "    def _build_feature_layers(self) -> nn.ModuleDict:\n",
    "        \"\"\"Build market-specific feature layers\"\"\"\n",
    "        if self.market_config.universe == \"USTOP100\":\n",
    "            return self._build_concentrated_market_layers()\n",
    "        elif self.market_config.universe == \"USTOP1000\":\n",
    "            return self._build_broad_market_layers()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
